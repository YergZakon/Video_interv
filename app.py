import streamlit as st
import cv2
import threading
import time
import numpy as np
import pyaudio
import queue
import os
from pathlib import Path
import matplotlib.pyplot as plt
import yaml

from modules.video_analyzer import VideoAnalyzer
from modules.audio_analyzer import AudioAnalyzer
from modules import MultimodalIntegrator
from modules.data_manager import QuestionManager
from utils.visualization import plot_voice_characteristics, plot_multimodal_timeline, create_distortion_summary

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
frame_queue = queue.Queue(maxsize=5)
audio_queue = queue.Queue(maxsize=5)
stop_event = threading.Event()
session_active = False

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –≤–∏–¥–µ–æ
def video_capture_thread(video_analyzer, integrator):
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    while not stop_event.is_set():
        ret, frame = cap.read()
        if ret:
            # –ê–Ω–∞–ª–∏–∑ –∫–∞–¥—Ä–∞
            processed_frame = video_analyzer.process_frame(frame)
            
            if not frame_queue.full():
                frame_queue.put(processed_frame)
                
            timestamp = time.time()
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∏–Ω—Ç–µ–≥—Ä–∞—Ç–æ—Ä
            video_result = {'frame': processed_frame}
            integrator.add_video_result(video_result, timestamp)
            
            time.sleep(0.03)  # ~30 FPS
    
    cap.release()

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –∞—É–¥–∏–æ
def audio_capture_thread(audio_analyzer, integrator):
    CHUNK = 1024
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000
    
    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT,
                    channels=CHANNELS,
                    rate=RATE,
                    input=True,
                    frames_per_buffer=CHUNK)
    
    while not stop_event.is_set():
        try:
            audio_data = stream.read(CHUNK)
            audio_chunk = np.frombuffer(audio_data, dtype=np.int16)
            
            if not audio_queue.full():
                audio_queue.put(audio_chunk)
            
            # –ê–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ
            audio_result = audio_analyzer.process_audio(audio_chunk)
            if audio_result:
                timestamp = time.time()
                integrator.add_audio_result(audio_result, timestamp)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞—Ö–≤–∞—Ç–µ –∞—É–¥–∏–æ: {e}")
            
    stream.stop_stream()
    stream.close()
    p.terminate()

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
def multimodal_analysis_thread(integrator):
    while not stop_event.is_set():
        current_time = time.time()
        result = integrator.analyze_multimodal(current_time)
        time.sleep(0.5)  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 0.5 —Å–µ–∫—É–Ω–¥

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
@st.cache_resource
def init_analyzers():
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    with open("config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    audio_config = config.get("audio", {})
    whisper_model = audio_config.get("whisper_model", "tiny")
    whisper_language = audio_config.get("whisper_language", "ru")

    video_analyzer = VideoAnalyzer()
    audio_analyzer = AudioAnalyzer(model_size=whisper_model, language=whisper_language)  # –ü–µ—Ä–µ–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏ —è–∑—ã–∫–∞
    integrator = MultimodalIntegrator(video_analyzer, audio_analyzer)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    data_dir = Path("data")
    if not data_dir.exists():
        data_dir.mkdir(parents=True)
    
    questions_path = data_dir / "questions.csv"
    if not questions_path.exists():
        # –ï—Å–ª–∏ —Ñ–∞–π–ª —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä
        import pandas as pd
        questions = pd.DataFrame({
            'id': range(1, 4),
            'text': [
                "–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –æ –≤–∞—à–µ–º –æ–ø—ã—Ç–µ —Ä–∞–±–æ—Ç—ã –≤ –∫–æ–º–∞–Ω–¥–µ",
                "–ö–∞–∫–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –≤—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º –ø—Ä–æ–µ–∫—Ç–µ?",
                "–ö–∞–∫ –≤—ã —Ä–µ—à–∞–µ—Ç–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏?"
            ],
            'category': ['–æ–±—â–∏–µ', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ', '–ª–∏—á–Ω–æ—Å—Ç–Ω—ã–µ']
        })
        questions.to_csv(questions_path, index=False)
    
    question_manager = QuestionManager(str(questions_path))
    
    return video_analyzer, audio_analyzer, integrator, question_manager

# –§—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —Å–µ—Å—Å–∏–∏
def start_session(video_analyzer, audio_analyzer, integrator):
    global stop_event, session_active
    
    if stop_event.is_set():
        stop_event.clear()
    
    # –°–±—Ä–æ—Å –±–∞–∑–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    video_analyzer.reset_baseline()
    audio_analyzer.reset_baseline()
    integrator.clear_history()
    
    # –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–æ–≤ –∑–∞—Ö–≤–∞—Ç–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞
    threading.Thread(target=video_capture_thread, args=(video_analyzer, integrator), daemon=True).start()
    threading.Thread(target=audio_capture_thread, args=(audio_analyzer, integrator), daemon=True).start()
    threading.Thread(target=multimodal_analysis_thread, args=(integrator,), daemon=True).start()
    
    session_active = True
    st.session_state.session_active = True

# –§—É–Ω–∫—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Å–µ—Å—Å–∏–∏
def stop_session():
    global stop_event, session_active
    stop_event.set()
    session_active = False
    st.session_state.session_active = False

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit
def main():
    st.set_page_config(
        page_title="–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–æ–≤",
        page_icon="üìä",
        layout="wide"
    )
    
    st.title("–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–æ–≤")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
    video_analyzer, audio_analyzer, integrator, question_manager = init_analyzers()
    
    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º
    with st.sidebar:
        st.header("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("–ù–∞—á–∞—Ç—å —Å–µ—Å—Å–∏—é", key="start_session", 
                        disabled=st.session_state.get('session_active', False)):
                start_session(video_analyzer, audio_analyzer, integrator)
        
        with col2:
            if st.button("–ó–∞–≤–µ—Ä—à–∏—Ç—å —Å–µ—Å—Å–∏—é", key="stop_session", 
                        disabled=not st.session_state.get('session_active', False)):
                stop_session()
        
        if st.button("–°–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å", key="next_question",
                    disabled=not st.session_state.get('session_active', False)):
            question = question_manager.next_question()
            if question is None:
                st.warning("–í–æ–ø—Ä–æ—Å—ã –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å. –°–µ—Å—Å–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
                stop_session()
        
        st.header("–¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å")
        current_question = question_manager.get_current_question()
        if current_question is not None:
            st.markdown(f"**{current_question['text']}**")
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            current, total, percent = question_manager.get_progress()
            st.progress(percent)
            st.text(f"–í–æ–ø—Ä–æ—Å {current} –∏–∑ {total}")
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å —Å –∞–Ω–∞–ª–∏–∑–æ–º
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("–í–∏–¥–µ–æ –∞–Ω–∞–ª–∏–∑")
        video_placeholder = st.empty()
    
    with col2:
        st.header("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        speech_container = st.container()
        with speech_container:
            st.subheader("–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–∞—è —Ä–µ—á—å")
            speech_placeholder = st.empty()
        
        indicators_container = st.container()
        with indicators_container:
            st.subheader("–ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã")
            indicators_placeholder = st.empty()
    
    # –†–∞–∑–¥–µ–ª —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
    st.header("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
    chart_col1, chart_col2 = st.columns(2)
    
    with chart_col1:
        st.subheader("–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≥–æ–ª–æ—Å–∞")
        voice_chart_placeholder = st.empty()
    
    with chart_col2:
        st.subheader("–í—Ä–µ–º–µ–Ω–Ω–∞—è —à–∫–∞–ª–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤")
        timeline_chart_placeholder = st.empty()
    
    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏
    st.header("–°–≤–æ–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    summary_placeholder = st.empty()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ UI
    update_ui(
        video_analyzer, audio_analyzer, integrator,
        video_placeholder, speech_placeholder, indicators_placeholder,
        voice_chart_placeholder, timeline_chart_placeholder, summary_placeholder
    )

def update_ui(
    video_analyzer, audio_analyzer, integrator,
    video_placeholder, speech_placeholder, indicators_placeholder,
    voice_chart_placeholder, timeline_chart_placeholder, summary_placeholder
):
    """–§—É–Ω–∫—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è UI"""
    
    pitch_values = []
    intensity_values = []
    timestamps = []
    last_speech_text = ""
    summary_counter = 0  # –°—á–µ—Ç—á–∏–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π
    
    while True:
        try:
            # –í—ã—Ö–æ–¥ –∏–∑ —Ü–∏–∫–ª–∞, –µ—Å–ª–∏ —Å–µ—Å—Å–∏—è –Ω–µ –∞–∫—Ç–∏–≤–Ω–∞
            if not st.session_state.get('session_active', False):
                time.sleep(0.1)
                continue
                
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ
            if not frame_queue.empty():
                frame = frame_queue.get()
                video_placeholder.image(frame, channels="BGR", use_column_width=True)
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∞—É–¥–∏–æ-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            audio_results = audio_analyzer.get_latest_results()
            if audio_results and 'text' in audio_results and audio_results['text']:
                last_speech_text = audio_results['text']
                speech_placeholder.markdown(f"*{last_speech_text}*")
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
                if 'pitch' in audio_results and 'intensity' in audio_results:
                    pitch_values.append(audio_results['pitch'])
                    intensity_values.append(audio_results['intensity'])
                    timestamps.append(time.time())
                    
                    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
                    max_points = 50
                    if len(pitch_values) > max_points:
                        pitch_values = pitch_values[-max_points:]
                        intensity_values = intensity_values[-max_points:]
                        timestamps = timestamps[-max_points:]
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏—Å–∫–∞–∂–µ–Ω–∏–π
            distortions = video_analyzer.get_distortions()
            if distortions:
                # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
                indicators_text = []
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏—Ü–µ–≤—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π
                for region, is_distorted in distortions.items():
                    status = "üî¥" if is_distorted else "üü¢"
                    indicators_text.append(f"{status} {region}")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π
                if audio_results and 'voice_change' in audio_results:
                    voice_status = "üî¥" if audio_results['voice_change'] else "üü¢"
                    indicators_text.append(f"{voice_status} –≥–æ–ª–æ—Å")
                
                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ
                indicators_placeholder.markdown("<br>".join(indicators_text), unsafe_allow_html=True)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
            if pitch_values and intensity_values:
                voice_fig = plot_voice_characteristics(pitch_values, intensity_values)
                if voice_fig:
                    voice_chart_placeholder.pyplot(voice_fig)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∫–∞–ª—ã
            recent_results = integrator.get_recent_results()
            if recent_results:
                timeline_fig = plot_multimodal_timeline(recent_results)
                if timeline_fig:
                    timeline_chart_placeholder.pyplot(timeline_fig)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏
                summary_text = create_distortion_summary(recent_results)
                summary_counter += 1  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–ª—é—á–∞
                summary_placeholder.text_area("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:", summary_text, height=200, key=f"summary_{summary_counter}")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ CPU
            time.sleep(0.1)
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞: {e}")
            time.sleep(1)

if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏
    if 'session_active' not in st.session_state:
        st.session_state.session_active = False
    
    main() 